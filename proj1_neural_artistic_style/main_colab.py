# -*- coding: utf-8 -*-
"""colab_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FqZ9Iiqe43kh6qWI_gP4Rumc-uu5cN--
"""

# imports
import numpy as np

import torch
from skimage import io, transform, color
import matplotlib.pyplot as plt
import torchvision
from torchvision import transforms, utils
from torchvision.utils import save_image

from PIL import Image

import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

device = ("cuda" if torch.cuda.is_available() else "cpu")


#load in content image
content_path = "./"
content_names = ['choatic.jpg']
content_image = Image.open(content_path+content_names[0])

#load in style image
style_path = "./"
style_names = ['soli.jpg']
style_image = Image.open(style_path+style_names[0])

plt.imshow(content_image)

style_arr = np.asarray(style_image)
style_means = [np.mean(style_arr[:,:,i]) for i in range(style_arr.shape[2])]
style_stds = [np.std(style_arr[:,:,i]) for i in range(style_arr.shape[2])]

content_arr = np.asarray(content_image)
content_means = [np.mean(content_arr[:,:,i]) for i in range(content_arr.shape[2])]
content_stds = [np.std(content_arr[:,:,i]) for i in range(content_arr.shape[2])]

#define and apply transforms
content_transform = transforms.Compose([transforms.Resize((512,512)),
            transforms.ToTensor()])#,
           # transforms.Normalize(content_means, content_stds)])

style_transform = transforms.Compose([transforms.Resize((512,512)),
            transforms.ToTensor()])#,
           # transforms.Normalize(style_means, style_stds)])

#transforms.Normalize([0.485, 0.456, 0.406],
                            #     [0.229, 0.224, 0.225])]

transforms = transforms.Compose([transforms.Resize((512,512)),
            transforms.ToTensor()])#,
             #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])
#how to know normalize

content_image = content_transform(content_image).unsqueeze(0) #unsqueeze to make it (1, 1, 512, 512)
style_image = content_transform(style_image).unsqueeze(0)


content_image = content_image.to(device)
style_image = style_image.to(device)

def convert_img(tensor):
    img = tensor.to("cpu").clone().detach()
    img = img.numpy().squeeze()
    img = img.transpose(1,2,0)
    img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
    img = np.clip(img,0,1)
    return img

def to_image(im):
  arr = np.asarray(im.cpu().squeeze().view(512,  512, -1))
  #arr = arr.reshape(512,512)
  return arr

content_image.shape

im = convert_img(content_image)
im.shape

plt.imshow(im) #ignore 3x3

vgg = models.vgg19(pretrained=True).features

avgPool = nn.AvgPool2d(kernel_size=2, stride=2)

for i, child in vgg.named_children():
  if isinstance(child, nn.MaxPool2d):
    vgg[int(i)] = avgPool

vgg.to(device).eval()

for param in vgg.parameters():
    param.requires_grad = False

vgg

content_layer = vgg[:23]

style_indices = [2,7, 12, 21, 31]
style_layers = []

for i in style_indices:
  style_layers.append(vgg[:i])

content_layer

def get_content_feature(x):
  return content_layer(x)

def get_style_features(x):
  style_features = [layer(x) for layer in style_layers]
  return style_features

#create white noise input
#mu, sigma = 0, 1 # mean and standard deviation
#s = np.random.normal(mu, sigma, size=(1, 3, 512, 512))
white_noise = torch.randn(1, 3, 512, 512, device=device, requires_grad=True)
white_noise = content_image
#white_noise = white_noise.cuda()
#white_noise = white_noise.to(device)

#white_noise = Variable(white_noise.cuda(), requires_grad=True)

print(white_noise)

content_image.shape

#get content and style images to compare  for loss
content_loss_feature = get_content_feature(content_image).detach()
content_loss_feature.to(device)

style_image
style_loss_features = get_style_features(style_image)
for i  in range(len(style_loss_features)):
  style_loss_features[i] = style_loss_features[i].detach()
  style_loss_features[i].to(device)

style_weight = 10e9 #beta  [10e3 or 10e4]
content_weight = 10e3 #alpha
num_iter = 5000

#create custom content and style losses

# Code adapted from https://www.javatpoint.com/pytorch-gram-matrix-for-style-transferring
#Initializing gram_matrix function for our tensor image
def gramMatrix(tensor):
    a,b,c,d = tensor.size()
    tensor = tensor.view(a*b,c*d)


    gram = torch.mm(tensor,tensor.t())

    #Returning gram matrix
    return gram


def contentLoss(noise):
    #use squared-error loss
    loss = F.mse_loss(noise.view(512, -1), content_loss_feature.view(512, -1)) * content_weight
    #loss = 0.5 * torch.sum(torch.pow(noise - content_loss_feature, 2)) *  content_weight

    return loss

gram_style_targets = [gramMatrix(style_feat) for style_feat in style_loss_features]

def styleLoss(noise_layers):
    total_style_loss = 0

    # get contribution of each layer to the total loss
    for i in range(len(noise_layers)):

        _,d,h,w=noise_layers[i].size()
        #print(d*h*w)
        gram_noise = gramMatrix(noise_layers[i])

        loss = F.mse_loss(gram_noise, gram_style_targets[i])
        #print(loss)
       # loss = 0.5 * torch.sum(torch.pow(gram_noise - gram_style_targets[i], 2)) *  style_weight


        total_style_loss += (loss / (d*h*w))

    return torch.mul(total_style_loss , style_weight)

#optimizer = optim.LBFGS(params = [white_noise], lr = 1,  max_iter=10)
optimizer = optim.Adam(params=[white_noise.requires_grad_(True)], lr=0.01)

# create the folder for the generated images
if not os.path.exists('./generated_2/'):
  os.mkdir('./generated_2/')

style_features = get_style_features(white_noise)
content_features =  get_content_feature(white_noise)

sl = styleLoss(style_features)
cl = contentLoss(content_features)

sl, cl

#style transfer loop
for num in range(num_iter+1):

      optimizer.zero_grad()

        #get current content layer and apply to noise image
      content_feature = get_content_feature(white_noise)

        #get content loss

      cl = contentLoss(content_feature).to(device)
        #print(cl)



        #get current style layers and apply them
      style_features = get_style_features(white_noise)

        #get style loss
      sl = styleLoss(style_features).to(device)
        # print(sl)

        #get total loss
      loss = cl + sl
      #loss.requires_grad = True
      #white_noise.retain_grad()
      #print(white_noise.grad)
        #loss.retain_grad()
        #print(loss.grad)
       # loss = nn.Parameter(loss_value, requires_grad = True)

        #back propagate loss
      #loss.backward()
     # print(loss)

      if num % 50 == 0:
          print("Iteration: {}, Content Loss: {:.3f}, Style Loss: {:.3f}".format(num, cl, sl, loss))

      # save intermediate  image
      if num % 500 == 0:
         save_image(white_noise.cpu().detach(), fp='./generated_2/iter_{}.png'.format(num))

      loss.backward()

      optimizer.step()
