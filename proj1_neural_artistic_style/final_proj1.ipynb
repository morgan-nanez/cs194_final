{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform, color\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "import dataLoader\n",
    "from dataLoader import PhotoDataset, Rescale, Rotate, ToTensor, ToGreyNormalize, ColorJitter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "\n",
    "#optimizer\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import torchvision.models as models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather data set\n",
    "\n",
    "#van-gogh from: https://www.kaggle.com/ipythonx/van-gogh-paintings\n",
    "\n",
    "image_names =  []\n",
    "path = \"training_data/van_gogh/\"\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        image_names.append(file)\n",
    "image_names = np.asarray(image_names)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"training_data/van_gogh/\"\n",
    "transformed_dataset_train = PhotoDataset(image_names = image_names,\n",
    "                                           root_dir=path,\n",
    "                                           transform= [#ToGreyNormalize(),\n",
    "                                                       Rescale((512,512)),\n",
    "                                                       ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"content_images/\"\n",
    "content_names = ['nutmeg.jpg']\n",
    "content_dataset = PhotoDataset(image_names = content_names,\n",
    "                                           root_dir=path,\n",
    "                                           transform= [#ToGreyNormalize(),\n",
    "                                                       transforms.Resize((512,512)),\n",
    "                                                       transforms.ToTensor(),\n",
    "                                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                             [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((512,512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],  \n",
    "                                 [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open content img\n",
    "content_img = Image.open(\"content_images/nutmeg.jpg\")\n",
    "content_image = transform(content_img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the images\n",
    "style_img = Image.open(\"training_data/van_gogh/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampled image from your dataloader visualized with ground-truth keypoints.\n",
    "random_indx = [2, 36, 47]\n",
    "for i in random_indx:\n",
    "    sample = transformed_dataset_train[i]\n",
    "    im = sample['image']\n",
    "    image = im.data\n",
    "    image = image.numpy()\n",
    "    s =  image.shape\n",
    "    print(s)\n",
    "    image = np.reshape(image, (s[1], s[2], 3))\n",
    "    #have to have this to save the image \n",
    "    implot = plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # load the vgg model's features\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "    \n",
    "    def get_content_activations(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Extracts the features for the content loss from the block4_conv2 of VGG19\n",
    "            Args:\n",
    "                x: torch.Tensor - input image we want to extract the features of\n",
    "            Returns:\n",
    "                features: torch.Tensor - the activation maps of the block4_conv2 layer\n",
    "        \"\"\"\n",
    "        features = self.vgg[:23](x)\n",
    "        return features\n",
    "    \n",
    "    def get_style_activations(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Extracts the features for the style loss from the block1_conv1, \n",
    "                block2_conv1, block3_conv1, block4_conv1, block5_conv1 of VGG19\n",
    "            Args:\n",
    "                x: torch.Tensor - input image we want to extract the features of\n",
    "            Returns:\n",
    "                features: list - the list of activation maps of the block1_conv1, \n",
    "                    block2_conv1, block3_conv1, block4_conv1, block5_conv1 layers\n",
    "        \"\"\"\n",
    "        features = [self.vgg[:4](x)] + [self.vgg[:7](x)] + [self.vgg[:12](x)] + [self.vgg[:21](x)] + [self.vgg[:30](x)] \n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.vgg(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load initial model\n",
    "vgg19 = models.vgg19(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save avgpool layer\n",
    "avgPool = vgg19.avgpool\n",
    "\n",
    "#only keep feature space\n",
    "vgg19 = vgg19.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change max pool layers to avg pool layers\n",
    "for i, child in vgg19.named_children():\n",
    "    if isinstance(child, nn.MaxPool2d):\n",
    "        vgg19[int(i)] = avgPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = (s[2], s[0], s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg19, sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationary feature extractor\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract content features (conv4_2)\n",
    "content_feature = vgg19[:22]\n",
    "\n",
    "#extract style features(conv1_1, conv2_1, conv3_1, conv4_1, conv5_1)\n",
    "style_features = [vgg19[:0]]+[vgg19[:5]]+[vgg19[:10]]+[vgg19[:19]]+[vgg19[:28]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_im.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content_act = vgg19[:22](content_image)\n",
    "content_act = content_act.view(512, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4922, -1.3637, -1.9718,  ...,  1.2022, -2.1436, -3.2753],\n",
       "        [ 0.0407, -0.7288, -1.4203,  ..., -3.3852, -2.1745, -0.3340],\n",
       "        [ 0.1702, -3.6247, -5.0771,  ..., -1.0185,  0.8282,  1.0640],\n",
       "        ...,\n",
       "        [ 4.0950,  3.9521,  2.3959,  ..., -2.3100, -1.6659,  0.9146],\n",
       "        [ 0.4880, -1.8757, -2.7719,  ..., -0.7039,  0.2067, -0.9594],\n",
       "        [-3.5972, -5.5392, -1.6151,  ..., -3.5265, -3.4930, -0.2222]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply changes to model\n",
    "relu = torch.nn.modules.activation.ReLU\n",
    "maxpool = torch.nn.modules.pooling.MaxPool2d\n",
    "new_features = []\n",
    "for feature in vgg19.features:\n",
    "    \n",
    "    # get rid of RELU layers\n",
    "    if type(feature) != relu:\n",
    "        \n",
    "        # change max_pool layers to avgpool\n",
    "        if  type(feature) == maxpool:\n",
    "            new_features.append(vgg19.avgpool)\n",
    "        else:\n",
    "            new_features.append(feature)\n",
    "            \n",
    "vgg19 = nn.Sequential(*new_features)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction loss\n",
    "criterion = nn.MSELoss() # mean squared error loss (torch.nn.MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = optim.Adam(vgg19.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation\n",
    "indices = list(range(len(transformed_dataset_train)))\n",
    "train_indices, val_indices = indices[:350], indices[350:]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "#Create Loaders\n",
    "train_loader = torch.utils.data.DataLoader(transformed_dataset_train, batch_size=1, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(transformed_dataset_train, batch_size=1,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "#train\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "    running_train_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    \n",
    "    #TRAINING\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        image = sample['image']\n",
    "        image = image.type(torch.FloatTensor)\n",
    "        print(image.shape)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = vgg19(image)\n",
    "        print(outputs.shape)\n",
    "        \n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs)\n",
    "        \n",
    "        #back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "     \n",
    "        \n",
    "    #store avg loss\n",
    "    loss_t = running_train_loss/len(train_loader)\n",
    "   # loss_v = running_val_loss/len(val_loader)\n",
    "    \n",
    "    training_loss.append(loss_t)\n",
    "   # validation_loss.append(loss_v)\n",
    "    print(\"Epoch: \" + str(epoch) + \" Training_Loss: \" + str(loss_t))# + \" Val_Loss: \" + str(loss_v))\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19\n",
    "for i, child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19[:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set content layers [conv4_1] [4 = block]\n",
    "content_layers = vgg[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, child in vgg19.named_children():\n",
    "    print(name, child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove fully connected layers(ie only feature space)\n",
    "#run the pre-trained model as a fixed feature extractor\n",
    "#and then use the resulting features to train a new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only want the feature space \n",
    "vgg19 = models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg19, (3, 600, 600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak the model\n",
    "#  - avgpool instead of max pool\n",
    "#  - #content reconstruction : conv1 1, conv2 1, conv3 1, conv4 1, conv5 1\n",
    "#  - #style reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher layers in the network capture the high-level content\n",
    "# feature responses in higher layers of the network is the content representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructions from the lower layers simply reproduce the exact pixel values \n",
    "# of the original image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create white noise image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for style representation\n",
    "# use gradient descent from a white noise image to find another image that \n",
    "# matches the style representation of the original image\n",
    "\n",
    "\n",
    "# minimising the mean-squared distance between the entries of the \n",
    "# Gram matrix from the original image \n",
    "# and the Gram matrix of the image to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix the content of a photograph with the style\n",
    "# jointly minimise the distance of a white noise \n",
    "# image from the content representation of the photograph in one layer of the network \n",
    "# and the style representation of the painting in a number of layers of the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# content reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform gradient descent on a white noise image to \n",
    "# - > find another image that matches the feature responses of the original image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
